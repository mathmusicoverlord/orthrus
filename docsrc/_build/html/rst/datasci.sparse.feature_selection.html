

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>datasci.sparse.feature_selection namespace &mdash; DataSci 1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="datasci.sparse namespace" href="datasci.sparse.html" />
    <link rel="prev" title="datasci.sparse.classifiers namespace" href="datasci.sparse.classifiers.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataSci
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="modules.html">datasci</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasci.core.html">datasci.core namespace</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasci.manifold.html">datasci.manifold namespace</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="datasci.sparse.html">datasci.sparse namespace</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="datasci.sparse.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="datasci.sparse.classifiers.html">datasci.sparse.classifiers namespace</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">datasci.sparse.feature_selection namespace</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-datasci.sparse.feature_selection.IterativeFeatureRemoval">datasci.sparse.feature_selection.IterativeFeatureRemoval module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-datasci.sparse.feature_selection.helper">datasci.sparse.feature_selection.helper module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-datasci.sparse.feature_selection.kffs">datasci.sparse.feature_selection.kffs module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-datasci.sparse.feature_selection.kfold_loso_ifr">datasci.sparse.feature_selection.kfold_loso_ifr module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-datasci.sparse.feature_selection.kfold_shuffle_ifr">datasci.sparse.feature_selection.kfold_shuffle_ifr module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-datasci.sparse.feature_selection.pathway_selection">datasci.sparse.feature_selection.pathway_selection module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-datasci.sparse.feature_selection.pca_snr_selection">datasci.sparse.feature_selection.pca_snr_selection module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-datasci.sparse.feature_selection.sabs">datasci.sparse.feature_selection.sabs module</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataSci</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="modules.html">datasci</a> &raquo;</li>
        
      <li>datasci.sparse.feature_selection namespace</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/rst/datasci.sparse.feature_selection.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="datasci-sparse-feature-selection-namespace">
<h1>datasci.sparse.feature_selection namespace<a class="headerlink" href="#datasci-sparse-feature-selection-namespace" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-datasci.sparse.feature_selection.IterativeFeatureRemoval">
<span id="datasci-sparse-feature-selection-iterativefeatureremoval-module"></span><h2>datasci.sparse.feature_selection.IterativeFeatureRemoval module<a class="headerlink" href="#module-datasci.sparse.feature_selection.IterativeFeatureRemoval" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR">
<em class="property">class </em><code class="sig-prename descclassname">datasci.sparse.feature_selection.IterativeFeatureRemoval.</code><code class="sig-name descname">IFR</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">repetition</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">partition_method</span><span class="o">=</span><span class="default_value">'stratified_k-fold'</span></em>, <em class="sig-param"><span class="n">nfolds</span><span class="o">=</span><span class="default_value">3</span></em>, <em class="sig-param"><span class="n">max_iters</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">cutoff</span><span class="o">=</span><span class="default_value">0.75</span></em>, <em class="sig-param"><span class="n">jumpratio</span><span class="o">=</span><span class="default_value">100.0</span></em>, <em class="sig-param"><span class="n">max_features_per_iter_ratio</span><span class="o">=</span><span class="default_value">0.8</span></em>, <em class="sig-param"><span class="n">verbosity</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>The Iterative Feature Removal algorithms extracts features for many data partitions independently and combines them and keep track of feature frequency, weights
and iterations in which each feature was extracted. During execution, the data is partitioned into training and validation sets ‘repetition’ number of times and
then for each partition, one feature set is extracted. So, a total of repetition * num_partitions independent feature sets are extracted and the results are
merged to create the output.</p>
<dl class="simple">
<dt>For each feature set, the algorithm can halt because of the following conditions:</dt><dd><ol class="arabic simple">
<li><p>BSR on validation partition is below cutoff</p></li>
<li><p>Jump does not occur in the array of sorted absolute weights</p></li>
<li><p>Jump occurs but the weight at the jump is too small ( &lt; 10e-6)</p></li>
<li><p>#selected features is greater than max_features_per_iter_ratio * #samples in training partition. This condition exists to prevent overfitting.</p></li>
<li><p>max_iters number of iterations complete successfully</p></li>
</ol>
</dd>
</dl>
<p>When one of these conditions happen, further feature extraction on the current fold is stopped and begins on the next partition.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>repetition</strong> (<em>int</em>) – Determines the number of times to partition the dataset. (default: 10)</p></li>
<li><p><strong>partition_method</strong> (<em>string</em>) – A partition method that is compatible with calcom.utils.generate_partitions (default: ‘stratified_k-fold’)</p></li>
<li><p><strong>nfolds</strong> (<em>int</em>) – The number of folds to partition data into (default: 3)</p></li>
<li><p><strong>max_iters</strong> (<em>int</em>) – Determines the maximum number of iterations of IFR on one data partition(default: 5)</p></li>
<li><p><strong>cutoff</strong> (<em>float</em>) – Threshold for the validation BSR (balanced success rate) to halt the process. (default: 0.75)</p></li>
<li><p><strong>jumpratio</strong> (<em>float</em>) – The relative drop in the magnitude of coefficients in weight vector to identify numerically zero weights (default: 100)</p></li>
<li><p><strong>max_features_per_iter_ratio</strong> (<em>float</em>) – 0.8)
if the number if selected features is greater than max_features_per_iter_ratio * #samples in training partition, further execution
on the current fold is stopped.</p></li>
<li><p><strong>verbosity</strong> (<em>int</em>) – Determines verbosity of print statments; 0 for no output; 2 for full output. (default: 0)</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.diagnostic_information_">
<code class="sig-name descname">diagnostic_information_</code><a class="headerlink" href="#datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.diagnostic_information_" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds execution information for each interation of each partition.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>The dataframe is indexed by feature_ids and contains the results of IFR. Each column contains different infromation for each feature as described below:</p>
<blockquote>
<div><p>frequency : How many times the feature is extracted
weights : Contains a list of weights, from the weight vectors during training on different partitions. Each value corresponds to the weight for the feature over different extractions.
The length of the weights is equal to the frequency.
selection_iteration : Contains a list of indices of the iteration when the feature was extracted over different data partitions. The length of the list is equal to the frequency.</p>
</div></blockquote>
</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>pandas.DataFrame</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.core.dataset</span> <span class="k">as</span> <span class="nn">DS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.IterativeFeatureRemoval</span> <span class="k">as</span> <span class="nn">IFR</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">DS</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ifr</span> <span class="o">=</span> <span class="n">IFR</span><span class="o">.</span><span class="n">IFR</span><span class="p">(</span>
<span class="go">    repetition = 500,</span>
<span class="go">    nfolds = 4,</span>
<span class="go">    max_iters = 100,</span>
<span class="go">    cutoff = .6,</span>
<span class="go">    jumpratio = 5,</span>
<span class="go">    max_features_per_iter_ratio = 2</span>
<span class="go">    verbosity = 2,</span>
<span class="go">    )</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">ifr</span><span class="p">,</span>
<span class="go">            attrname,</span>
<span class="go">            selector_name=&#39;IFR&#39;,</span>
<span class="go">            f_results_handle=&#39;results&#39;,</span>
<span class="go">            append_to_meta=False,</span>
<span class="go">            )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#see feature select method for details</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ifr</span><span class="o">.</span><span class="n">plot_basic_diagnostic_stats</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt id="datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="n">labels</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>data        : m-by-n array of data, with m the number of observations.
labels      : m vector of labels for the data</p>
<p>return      : a dictionary of features {keys=feature : value=no_of_time_it_was_selected}</p>
</dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.plot_basic_diagnostic_stats">
<code class="sig-name descname">plot_basic_diagnostic_stats</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">validation_bsr_iteration_idx</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_random_exp</span><span class="o">=</span><span class="default_value">- 1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.plot_basic_diagnostic_stats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.helper">
<span id="datasci-sparse-feature-selection-helper-module"></span><h2>datasci.sparse.feature_selection.helper module<a class="headerlink" href="#module-datasci.sparse.feature_selection.helper" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="datasci.sparse.feature_selection.helper.rank_features_by_attribute">
<code class="sig-prename descclassname">datasci.sparse.feature_selection.helper.</code><code class="sig-name descname">rank_features_by_attribute</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">features_df</span></em>, <em class="sig-param"><span class="n">args</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.helper.rank_features_by_attribute" title="Permalink to this definition">¶</a></dt>
<dd><p>This method takes a features dataframe as input and ranks them based on a column/attribute which contains numerical data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features_df</strong> (<em>pandas.DataFrame</em>) – This is a features dataframe that contains result of a feature selection.
(check datasci.core.dataset.DataSet.feature_select method for details)</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>This dictionary contains variables to determine which attribute to rank feature on and the
order of ranking. Check details for various key and values below:
‘attr’ (Mandatory): Attribute/ Column name in the features_df to rank the features on
‘order’: Whether to rank in ascending or descending order. ‘asc’ for ascending and ‘desc’ for descending.</p>
<blockquote>
<div><p>(defaul: ‘desc’)</p>
</div></blockquote>
<dl class="simple">
<dt>’feature_ids’ (list-like): To limit the ranking within certain features. List of indicators for the features to use.</dt><dd><p>e.g. [True, False, True], [‘gene1’, ‘gene3’], etc…, can also be pandas series or numpy array.
Default: None which corresponds to using all features.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>array of sorted features (index of features_df)</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.core.dataset</span> <span class="k">as</span> <span class="nn">DS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.IterativeFeatureRemoval</span> <span class="k">as</span> <span class="nn">IFR</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">DS</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ifr</span> <span class="o">=</span> <span class="n">IFR</span><span class="o">.</span><span class="n">IFR</span><span class="p">(</span>
<span class="go">    verbosity = 2,</span>
<span class="go">    nfolds = 4,</span>
<span class="go">    repetition = 500,</span>
<span class="go">    cutoff = .6,</span>
<span class="go">    jumpratio = 5,</span>
<span class="go">    max_iters = 100,</span>
<span class="go">    max_features_per_iter_ratio = 2</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">ifr</span><span class="p">,</span>
<span class="go">            attrname,</span>
<span class="go">            selector_name=&#39;IFR&#39;,</span>
<span class="go">            f_results_handle=&#39;results&#39;,</span>
<span class="go">            append_to_meta=False,</span>
<span class="go">            )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features_df</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;f_results&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ranking_method_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;attr&#39;</span><span class="p">:</span> <span class="s1">&#39;frequency&#39;</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ranked_order</span> <span class="o">=</span>  <span class="n">rank_features_by_attribute</span><span class="p">(</span><span class="n">features_df</span><span class="p">,</span> <span class="n">ranking_method_args</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="datasci.sparse.feature_selection.helper.rank_features_by_mean_attribute_value">
<code class="sig-prename descclassname">datasci.sparse.feature_selection.helper.</code><code class="sig-name descname">rank_features_by_mean_attribute_value</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">features_df</span></em>, <em class="sig-param"><span class="n">args</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.helper.rank_features_by_mean_attribute_value" title="Permalink to this definition">¶</a></dt>
<dd><p>CHECK THIS BEFORE COMMITING
This method takes a features dataframe as input and ranks them based on a column/attribute which contains numerical data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features_df</strong> (<em>pandas.DataFrame</em>) – This is a features dataframe that contains result of a feature selection.
(check datasci.core.dataset.DataSet.feature_select method for details)</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>This dictionary contains variables to determine which attribute to rank feature on and the
order of ranking. Check details for various key and values below:
‘attr’ (Mandatory): Attribute/ Column name in the features_df to rank the features on
‘order’: Whether to rank in ascending or descending order. ‘asc’ for ascending and ‘desc’ for descending.</p>
<blockquote>
<div><p>(defaul: ‘desc’)</p>
</div></blockquote>
<dl class="simple">
<dt>’feature_ids’ (list-like): To limit the ranking within certain features. List of indicators for the features to use.</dt><dd><p>e.g. [True, False, True], [‘gene1’, ‘gene3’], etc…, can also be pandas series or numpy array.
Default: None which corresponds to using all features.</p>
</dd>
</dl>
<p>’method’: which operation to perform. Can be “mean” or “median”. (Default: “mean”)</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>array of sorted features (index of features_df)</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.core.dataset</span> <span class="k">as</span> <span class="nn">DS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.IterativeFeatureRemoval</span> <span class="k">as</span> <span class="nn">IFR</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">DS</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ifr</span> <span class="o">=</span> <span class="n">IFR</span><span class="o">.</span><span class="n">IFR</span><span class="p">(</span>
<span class="go">    verbosity = 2,</span>
<span class="go">    nfolds = 4,</span>
<span class="go">    repetition = 500,</span>
<span class="go">    cutoff = .6,</span>
<span class="go">    jumpratio = 5,</span>
<span class="go">    max_iters = 100,</span>
<span class="go">    max_features_per_iter_ratio = 2</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">ifr</span><span class="p">,</span>
<span class="go">            attrname,</span>
<span class="go">            selector_name=&#39;IFR&#39;,</span>
<span class="go">            f_results_handle=&#39;results&#39;,</span>
<span class="go">            append_to_meta=False,</span>
<span class="go">            )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features_df</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;f_results&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ranking_method_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;attr&#39;</span><span class="p">:</span> <span class="s1">&#39;frequency&#39;</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ranked_order</span> <span class="o">=</span>  <span class="n">rank_features_by_attribute</span><span class="p">(</span><span class="n">features_df</span><span class="p">,</span> <span class="n">ranking_method_args</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="datasci.sparse.feature_selection.helper.rank_features_within_attribute_class">
<code class="sig-prename descclassname">datasci.sparse.feature_selection.helper.</code><code class="sig-name descname">rank_features_within_attribute_class</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">features_df</span></em>, <em class="sig-param"><span class="n">feature_class_attribute</span></em>, <em class="sig-param"><span class="n">new_feature_attribute_name</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">partitioner</span></em>, <em class="sig-param"><span class="n">sample_ids</span></em>, <em class="sig-param"><span class="n">scorer</span></em>, <em class="sig-param"><span class="n">classification_attr</span></em>, <em class="sig-param"><span class="n">classifier_factory_method</span></em>, <em class="sig-param"><span class="n">f_weights_handle</span></em>, <em class="sig-param"><span class="n">feature_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.helper.rank_features_within_attribute_class" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="datasci.sparse.feature_selection.helper.reduce_feature_set_size">
<code class="sig-prename descclassname">datasci.sparse.feature_selection.helper.</code><code class="sig-name descname">reduce_feature_set_size</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ds</span></em>, <em class="sig-param"><span class="n">features_dataframe</span></em>, <em class="sig-param"><span class="n">sample_ids</span></em>, <em class="sig-param"><span class="n">attr</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">classifier_factory_method_handle</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">scorer</span></em>, <em class="sig-param"><span class="n">ranking_method_handle</span></em>, <em class="sig-param"><span class="n">ranking_method_args</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">partitioner</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">test_sample_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">start</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">end</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">jump</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.helper.reduce_feature_set_size" title="Permalink to this definition">¶</a></dt>
<dd><p>This method takes a features dataframe (outut of a feature selection), ranks them by a ranking method and performs
a feature set reduction using grid search method which is defined by start, end and jump parameters.</p>
<p>Different training and test data may be used and the results will change accordingly. The following choices are avaible:</p>
<dl class="simple">
<dt>if test_sample_ids is None and partitioner is None:</dt><dd><p>Only training data is available, so the results will contain score on the Training data</p>
</dd>
<dt>if test_sample_ids is None and partitioner is not None:</dt><dd><p>Model is trained using partitions of sample_ids defined by partitioner, results contain the mean test score obtained during classification on these partitions</p>
</dd>
<dt>if test_sample_ids is not None and partitioner is None:</dt><dd><p>Model is trained on all sample_ids, and then it is then evaluated on test_sample_ids. Results contain evaluation score on test_sample_ids</p>
</dd>
<dt>if test_sample_ids is not None and partitioner is not None:</dt><dd><p>Model is trained using partitions of sample_ids defined by partitioner, and the best model is then evaluated on test_sample_ids. Results
contain evaluation score on test_sample_ids</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features_df</strong> (<em>pandas.DataFrame</em>) – This is a features dataframe that contains result of a feature selection.
(check datasci.core.dataset.DataSet.feature_select method for details)</p></li>
<li><p><strong>sample_ids</strong> (<em>like-like</em>) – List of indicators for the samples to use for training. e.g. [1,3], [True, False, True],
[‘human1’, ‘human3’], etc…, can also be pandas series or numpy array.</p></li>
<li><p><strong>attr</strong> (<em>string</em>) – Name of metadata attribute to classify on.</p></li>
<li><p><strong>scorer</strong> (<em>object</em>) – Function which scores the prediction labels on training and test partitions. This function
should accept two arguments: truth labels and prediction labels. This function should output a score
between 0 and 1 which can be thought of as an accuracy measure. See
sklearn.metrics.balanced_accuracy_score for an example.</p></li>
<li><p><strong>ranking_method_handle</strong> (<em>method handle</em>) – handle of the feature ranking method</p></li>
<li><p><strong>ranking_method_args</strong> (<em>dict</em>) – argument dictionary for the feature ranking method</p></li>
<li><p><strong>partitioner</strong> (<em>object</em>) – Class-instance which partitions samples in batches of training and test split. This</p></li>
<li><p><strong>must have the sklearn equivalent of a split method. The split method returns a list of</strong> (<em>instance</em>) – </p></li>
<li><p><strong>partitions; one for each fold in the experiment. See sklearn.model_selection.KFold for</strong> (<em>train-test</em>) – </p></li>
<li><p><strong>example partitioner.</strong> (<em>an</em>) – </p></li>
<li><p><strong>test_sample_ids</strong> – List of indicators for the samples to use for testing. e.g. [1,3], [True, False, True],
[‘human1’, ‘human3’], etc…, can also be pandas series or numpy array. (default = None, check the method
description above to see how this affects the results)</p></li>
<li><p><strong>start</strong> (<em>int</em>) – starting point of the grid search. (default: 5)</p></li>
<li><p><strong>end</strong> (<em>int</em>) – end point of the grid search. Use -1 to set end as the size of features (default: 100)</p></li>
<li><p><strong>jump</strong> (<em>int</em>) – gap between each sampled point in the grid (default: 5)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>‘optimal_n_results’: an array of m x 2 values, with m being the total values sampled from the grid in the search.
The first column contains the number of top features (different values sampled from the grid search),  and the second
column contains the score.  The array is sorted by score in descending order.</p>
<p>’reduced_feature_ids’ : array of reduced features ids(index of features_df). The size of the reduced feature set is the
smallest value n, out of the m sampled values, that produces the highest score.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>a dictionary that contains 2 key-value pairs</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.core.dataset</span> <span class="k">as</span> <span class="nn">DS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.IterativeFeatureRemoval</span> <span class="k">as</span> <span class="nn">IFR</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">DS</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ifr</span> <span class="o">=</span> <span class="n">IFR</span><span class="o">.</span><span class="n">IFR</span><span class="p">(</span>
<span class="go">    verbosity = 2,</span>
<span class="go">    nfolds = 4,</span>
<span class="go">    repetition = 500,</span>
<span class="go">    cutoff = .6,</span>
<span class="go">    jumpratio = 5,</span>
<span class="go">    max_iters = 100,</span>
<span class="go">    max_features_per_iter_ratio = 2</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">ifr</span><span class="p">,</span>
<span class="go">            attrname,</span>
<span class="go">            selector_name=&#39;IFR&#39;,</span>
<span class="go">            f_results_handle=&#39;results&#39;,</span>
<span class="go">            append_to_meta=False,</span>
<span class="go">            )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features_df</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;f_results&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">model_factory</span><span class="p">():</span>
<span class="go">        return svm.LinearSVC(dual=False)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bsr</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ranking_method_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;attr&#39;</span><span class="p">:</span> <span class="s1">&#39;frequency&#39;</span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">partitioner</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.helper</span> <span class="k">as</span> <span class="nn">fhelper</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reduced_feature_results</span> <span class="o">=</span> <span class="n">fhelper</span><span class="o">.</span><span class="n">reduce_feature_set_size</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
<span class="go">                        features_df,</span>
<span class="go">                        sample_ids_training,</span>
<span class="go">                        attrname,</span>
<span class="go">                        model_factory,</span>
<span class="go">                        bsr,</span>
<span class="go">                        fhelper.rank_features_by_attribute,</span>
<span class="go">                        ranking_method_args,</span>
<span class="go">                        test_sample_ids=sample_ids_validation,</span>
<span class="go">                        start = 5,</span>
<span class="go">                        end = 100,</span>
<span class="go">                        jump = 1)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">reduced_feature_results</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.kffs">
<span id="datasci-sparse-feature-selection-kffs-module"></span><h2>datasci.sparse.feature_selection.kffs module<a class="headerlink" href="#module-datasci.sparse.feature_selection.kffs" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.kffs.KFFS">
<em class="property">class </em><code class="sig-prename descclassname">datasci.sparse.feature_selection.kffs.</code><code class="sig-name descname">KFFS</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">k</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">n</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">classifier</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">f_weights_handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">f_rnk_func</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">training_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kffs.KFFS" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>K-fold Feature Selection (kFFS) selects features using a classifier in a k-fold cross-validation experiment.
Specifically, the given classifier is trained on each fold, the features in each fold are ranked and sorted,
the top n features are selected from each fold, and the features across all folds are collected and ranked
by how many times a given feature was in the top n across each fold.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k</strong> (<em>int</em>) – Indicates the number of folds to use in k-fold partition. Default is 5.</p></li>
<li><p><strong>n</strong> – (int): Indicates the rank threshold to use for each fold. KFFS grabs the top <code class="docutils literal notranslate"><span class="pre">n</span></code> features from each fold.</p></li>
<li><p><strong>classifier</strong> (<em>object</em>) – Class instance of the classifier being used, must contain a <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p></li>
<li><p><strong>f_weights_handle</strong> (<em>str</em>) – Name of the classifier attribute containing the feature weights for a given fold.</p></li>
<li><p><strong>f_rnk_func</strong> (<em>object</em>) – Function to be applied to feature weights for feature ranking. Default is None, and the
features will be ranked in from least to greatest.</p></li>
<li><p><strong>training_ids</strong> (<em>list</em>) – Optional list of training ids, to restrict feature selection further.</p></li>
<li><p><strong>random_state</strong> (<em>int</em>) – Random state to generate k-fold partitions. Default is 0.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.kffs.KFFS.classifiers_">
<code class="sig-name descname">classifiers_</code><a class="headerlink" href="#datasci.sparse.feature_selection.kffs.KFFS.classifiers_" title="Permalink to this definition">¶</a></dt>
<dd><p>Contains each classifier trained on each fold.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>series</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.kffs.KFFS.ranks_">
<code class="sig-name descname">ranks_</code><a class="headerlink" href="#datasci.sparse.feature_selection.kffs.KFFS.ranks_" title="Permalink to this definition">¶</a></dt>
<dd><p>Contains the final rankings of all the features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.kffs.KFFS.results_">
<code class="sig-name descname">results_</code><a class="headerlink" href="#datasci.sparse.feature_selection.kffs.KFFS.results_" title="Permalink to this definition">¶</a></dt>
<dd><p>Contains the feature weights and ranks for each fold, and the final rankings.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.kffs.KFFS.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kffs.KFFS.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the kFFS model to the training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>(</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training samples to be used for feature selection.</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>(</em><em>n_features</em><em>,</em><em>)</em>) – Training labels to be used for feature selection.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>inplace method. Results are stored in <a class="reference internal" href="#datasci.sparse.feature_selection.kffs.KFFS.classifiers_" title="datasci.sparse.feature_selection.kffs.KFFS.classifiers_"><code class="xref py py-attr docutils literal notranslate"><span class="pre">KFFS.classifiers_</span></code></a>, <a class="reference internal" href="#datasci.sparse.feature_selection.kffs.KFFS.ranks_" title="datasci.sparse.feature_selection.kffs.KFFS.ranks_"><code class="xref py py-attr docutils literal notranslate"><span class="pre">KFFS.ranks_</span></code></a>, and <a class="reference internal" href="#datasci.sparse.feature_selection.kffs.KFFS.results_" title="datasci.sparse.feature_selection.kffs.KFFS.results_"><code class="xref py py-attr docutils literal notranslate"><span class="pre">KFFS.results_</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.kfold_loso_ifr">
<span id="datasci-sparse-feature-selection-kfold-loso-ifr-module"></span><h2>datasci.sparse.feature_selection.kfold_loso_ifr module<a class="headerlink" href="#module-datasci.sparse.feature_selection.kfold_loso_ifr" title="Permalink to this headline">¶</a></h2>
<p>This module contains code for implementing an iterative feature removal (IFR) algorithm using leave one
subject out (LOSO) partitions.</p>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR">
<em class="property">class </em><code class="sig-prename descclassname">datasci.sparse.feature_selection.kfold_loso_ifr.</code><code class="sig-name descname">KFLIFR</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">classifier</span><span class="p">:</span> <span class="n">object</span></em>, <em class="sig-param"><span class="n">weights_handle</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">n_splits_kfold</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">random_state_kfold</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">train_test_splits</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">n_top_features</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">jump_ratio</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sort_freq_classes</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">imputer</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>This iterative feature removal (IFR) algorithm produces ranked feature sets from a single classifier and a data set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>classifier</strong> (<em>object</em>) – The classifier used to select features. The classifier should produce feature weights and ideally
be sparse, so that relatively few features are weighted heavily compared to the total.</p></li>
<li><p><strong>weights_handle</strong> (<em>str</em>) – The name of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.classifier</span></code> attribute where the weights are stored. The
weights stored there should be an ndarray.</p></li>
<li><p><strong>n_splits_kfold</strong> (<em>int</em>) – The number of folds, <code class="docutils literal notranslate"><span class="pre">k</span></code> used in the k-fold cross validation.</p></li>
<li><p><strong>random_state_kfold</strong> (<em>int</em>) – The random seed used in generating the k-fold partitions. Good for reproducibility
of results. The default is None.</p></li>
<li><p><strong>train_test_splits</strong> (<em>list</em>) – An alternate list of (training, test) splits that replace the k-fold cross validation used
in the algorithm. This is useful if you have a specific set of splits you want to use. The default is None,
but if provided the arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.n_splits_kfold</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.random_state_kfold</span></code> are
ignored.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – The proportion of features used to break the IFR loop. The IFR loop will stop once the number of
features extracted reaches this proportion of the total features.</p></li>
<li><p><strong>n_top_features</strong> (<em>int</em>) – The number of top features to remove at each iteration of IFR. If this parameter is not
given then <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.jump_ratio</span></code> will be used instead. The default is None.</p></li>
<li><p><strong>jump_ratio</strong> (<em>float</em>) – The weight ratio used to determine the number of top features to remove for each iteration
of IFR. For example let <span class="math notranslate nohighlight">\(r_i = w_{i}/w_{i+1}\)</span> denote the ratio of the
<span class="math notranslate nohighlight">\(i\)</span> th largest weight and <span class="math notranslate nohighlight">\(i+1\)</span> largest weight, then <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.jump_ratio</span></code> = 2 means
that top features will be chosen until <span class="math notranslate nohighlight">\(r_i \geq 2\)</span>. If this parameter is not given then the user
must provide a constant number of features to prune at each step via the parameter <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.n_top_features</span></code>.</p></li>
<li><p><strong>sort_freq_classes</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> the algorithm will sort frequency classes of feature by the mean of
normalized weight across a LOSO experiment— providing a unique ranking. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>imputer</strong> (<em>object</em>) – Optional imputer to impute training set values with.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.results_">
<code class="sig-name descname">results_</code><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.results_" title="Permalink to this definition">¶</a></dt>
<dd><p>Outputs the feature frequencies and rankings for each fold provided by the
k-fold partition defined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.n_splits_kfold</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.random_state_kfold</span></code>, or
the user defined splits defined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.train_test_splits</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">groups</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the feature selection algorithm and stores the results in the attribute <a class="reference internal" href="#datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.results_" title="datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.results_"><code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.results_</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The data array used to select features via <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.classifier</span></code>.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>)</em>) – The labels used to select features via <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.classifier</span></code>.</p></li>
<li><p><strong>groups</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>)</em>) – Optional set up of labels defining the groups used in a
Leave-One-Group-Out experiment. This is useful if you don’t want members of the same group in both the
training in test for a LOSO fold.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>inplace method.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.kfold_shuffle_ifr">
<span id="datasci-sparse-feature-selection-kfold-shuffle-ifr-module"></span><h2>datasci.sparse.feature_selection.kfold_shuffle_ifr module<a class="headerlink" href="#module-datasci.sparse.feature_selection.kfold_shuffle_ifr" title="Permalink to this headline">¶</a></h2>
<p>This module contains code for implementing an iterative feature removal (IFR) algorithm using leave one
subject out (LOSO) partitions.</p>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR">
<em class="property">class </em><code class="sig-prename descclassname">datasci.sparse.feature_selection.kfold_shuffle_ifr.</code><code class="sig-name descname">KFSIFR</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">classifier</span><span class="p">:</span> <span class="n">object</span></em>, <em class="sig-param"><span class="n">weights_handle</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">n_splits_kfold</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">random_state_kfold</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">train_test_splits</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.6</span></em>, <em class="sig-param"><span class="n">max_feature_threshold</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_splits_shuffle</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">random_state_shuffle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">train_prop_shuffle</span><span class="o">=</span><span class="default_value">0.8</span></em>, <em class="sig-param"><span class="n">n_top_features</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">jump_ratio</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sort_freq_classes</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">imputer</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>This iterative feature removal (IFR) algorithm produces ranked feature sets from a single classifier and a data set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>classifier</strong> (<em>object</em>) – The classifier used to select features. The classifier should produce feature weights and ideally
be sparse, so that relatively few features are weighted heavily compared to the total.</p></li>
<li><p><strong>weights_handle</strong> (<em>str</em>) – The name of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.classifier</span></code> attribute where the weights are stored. The
weights stored there should be an ndarray.</p></li>
<li><p><strong>n_splits_kfold</strong> (<em>int</em>) – The number of folds, <code class="docutils literal notranslate"><span class="pre">k</span></code> used in the k-fold cross validation.</p></li>
<li><p><strong>random_state_kfold</strong> (<em>int</em>) – The random seed used in generating the k-fold partitions. Good for reproducibility
of results. The default is None.</p></li>
<li><p><strong>train_test_splits</strong> (<em>list</em>) – An alternate list of (training, test) splits that replace the k-fold cross validation used
in the algorithm. This is useful if you have a specific set of splits you want to use. The default is None,
but if provided the arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.n_splits_kfold</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.random_state_kfold</span></code> are
ignored.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – Classification rate used to break the IFR loop. The IFR loop will stop once the number of
features extracted reaches this proportion of the total features.</p></li>
<li><p><strong>max_feature_threshold</strong> (<em>int</em>) – The maximum number of features that may be removed on an iteration of IFR.
Default is None.</p></li>
<li><p><strong>n_splits_shuffle</strong> (<em>int</em>) – The number of shuffle splits to use for the inner loop. Default is 100.</p></li>
<li><p><strong>random_state_shuffle</strong> (<em>int</em>) – Random seed for shuffle splits.</p></li>
<li><p><strong>n_top_features</strong> (<em>int</em>) – The number of top features to remove at each iteration of IFR. If this parameter is not
given then <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.jump_ratio</span></code> will be used instead. The default is None.</p></li>
<li><p><strong>jump_ratio</strong> (<em>float</em>) – The weight ratio used to determine the number of top features to remove for each iteration
of IFR. For example let <span class="math notranslate nohighlight">\(r_i = w_{i}/w_{i+1}\)</span> denote the ratio of the
<span class="math notranslate nohighlight">\(i\)</span> th largest weight and <span class="math notranslate nohighlight">\(i+1\)</span> largest weight, then <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.jump_ratio</span></code> = 2 means
that top features will be chosen until <span class="math notranslate nohighlight">\(r_i \geq 2\)</span>. If this parameter is not given then the user
must provide a constant number of features to prune at each step via the parameter <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.n_top_features</span></code>.</p></li>
<li><p><strong>sort_freq_classes</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> the algorithm will sort frequency classes of feature by the mean of
normalized weight across a LOSO experiment— providing a unique ranking. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>imputer</strong> (<em>object</em>) – Optional imputer to impute training set values with.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.results_">
<code class="sig-name descname">results_</code><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.results_" title="Permalink to this definition">¶</a></dt>
<dd><p>Outputs the feature frequencies and rankings for each fold provided by the
k-fold partition defined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.n_splits_kfold</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.random_state_kfold</span></code>, or
the user defined splits defined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.train_test_splits</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">groups</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the feature selection algorithm and stores the results in the attribute <a class="reference internal" href="#datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.results_" title="datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.results_"><code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.results_</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The data array used to select features via <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.classifier</span></code>.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>)</em>) – The labels used to select features via <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.classifier</span></code>.</p></li>
<li><p><strong>groups</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>)</em>) – Optional set up of labels defining the groups used in a
Leave-One-Group-Out experiment. This is useful if you don’t want members of the same group in both the
training in test for a LOSO fold.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>inplace method.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.pathway_selection">
<span id="datasci-sparse-feature-selection-pathway-selection-module"></span><h2>datasci.sparse.feature_selection.pathway_selection module<a class="headerlink" href="#module-datasci.sparse.feature_selection.pathway_selection" title="Permalink to this headline">¶</a></h2>
<p>Module containing classes for pathway discovery in -omics data.</p>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.pathway_selection.PathwayScore">
<em class="property">class </em><code class="sig-prename descclassname">datasci.sparse.feature_selection.pathway_selection.</code><code class="sig-name descname">PathwayScore</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">use_ray</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pathway_selection.PathwayScore" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<dl class="py method">
<dt id="datasci.sparse.feature_selection.pathway_selection.PathwayScore.convert_type">
<code class="sig-name descname">convert_type</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pathway_selection.PathwayScore.convert_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.pathway_selection.PathwayScore.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">pathways</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pathway_selection.PathwayScore.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.pathway_selection.PathwayScore.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pathway_selection.PathwayScore.transform" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.pca_snr_selection">
<span id="datasci-sparse-feature-selection-pca-snr-selection-module"></span><h2>datasci.sparse.feature_selection.pca_snr_selection module<a class="headerlink" href="#module-datasci.sparse.feature_selection.pca_snr_selection" title="Permalink to this headline">¶</a></h2>
<p>This module contains a class which implements the PCA-SNR feature selection method.</p>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.pca_snr_selection.PCASNR">
<em class="property">class </em><code class="sig-prename descclassname">datasci.sparse.feature_selection.pca_snr_selection.</code><code class="sig-name descname">PCASNR</code><a class="headerlink" href="#datasci.sparse.feature_selection.pca_snr_selection.PCASNR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<dl class="py method">
<dt id="datasci.sparse.feature_selection.pca_snr_selection.PCASNR.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pca_snr_selection.PCASNR.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.pca_snr_selection.PCASNR.plot_snrs">
<code class="sig-name descname">plot_snrs</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pca_snr_selection.PCASNR.plot_snrs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.pca_snr_selection.PCASNR.snr">
<code class="sig-name descname">snr</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pca_snr_selection.PCASNR.snr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.sabs">
<span id="datasci-sparse-feature-selection-sabs-module"></span><h2>datasci.sparse.feature_selection.sabs module<a class="headerlink" href="#module-datasci.sparse.feature_selection.sabs" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.sabs.SABS">
<em class="property">class </em><code class="sig-prename descclassname">datasci.sparse.feature_selection.sabs.</code><code class="sig-name descname">SABS</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">a</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">b</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">iterations</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">store_updates</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">use_cuda</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">device</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.sabs.SABS" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<dl class="py method">
<dt id="datasci.sparse.feature_selection.sabs.SABS.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.sabs.SABS.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="datasci.sparse.html" class="btn btn-neutral float-right" title="datasci.sparse namespace" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="datasci.sparse.classifiers.html" class="btn btn-neutral float-left" title="datasci.sparse.classifiers namespace" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Eric Kehoe.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>